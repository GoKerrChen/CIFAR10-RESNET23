# -*- coding: utf-8 -*-
"""ResNet23.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12tk2rSzPinwOuGmxoYXFVG6GxW9u9ftc
"""

# Commented out IPython magic to ensure Python compatibility.
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import pandas as pd
import numpy as np
from torchvision.utils import make_grid
import matplotlib.pyplot as plt
# %matplotlib inline

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

train_transform = transforms.Compose([#Compose multi transform functions
              transforms.RandomCrop(size = (32, 32), padding = 4)  ,#Random copping
              transforms.RandomHorizontalFlip(),#Random horizontal flip
              transforms.ToTensor(),
              transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],std=[0.2471, 0.2435, 0.2616])                         
])

test_transform = transforms.Compose([
              transforms.ToTensor(),
              transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],std=[0.2471, 0.2435, 0.2616])
])

train_data = datasets.CIFAR10(root='data', train=True, transform=train_transform, download=True)
test_data = datasets.CIFAR10(root='data', train=False, transform=test_transform)

valid_transform = test_transform
valid_data = test_data

train_data

test_data

batch_size = 128
train_loader = DataLoader(dataset=train_data, batch_size=128, shuffle=True)
test_loader = DataLoader(dataset=test_data, batch_size=128, shuffle=False)
valid_loader = test_loader

class_names = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']
np.set_printoptions(formatter=dict(int=lambda x: f'{x:5}'))#to widen the printed array
#Grab the first batch of 10 images
for images, label in train_loader:
  break

#print the labels
print('label:', label.numpy())
print('class:', *np.array([class_names[i] for i in label]))

#print the images
im = make_grid(images, nrow = 10)#the default nrow is 8
plt.figure(figsize=(10,4))
plt.imshow(np.transpose(im.numpy(), (1, 2, 0)))

"""### Define Residual Block"""

import time
from torch import nn, optim
class Residual(nn.Module):
  def __init__(self, in_channels, out_channels, use_1x1conv=False, stride=1):
    super(Residual, self).__init__()
    self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=(3,3), padding=1, stride=stride)
    self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=(3,3), padding=1, stride=1)
    #second conv layer has s=1 but first can be 2 sometimes
    if use_1x1conv:
      self.conv3 = nn.Conv2d(in_channels, out_channels, kernel_size=(1,1), stride=stride)
    else:
      self.conv3 = None
    
    self.bn1 = nn.BatchNorm2d(out_channels)
    self.bn2 = nn.BatchNorm2d(out_channels)

  def forward(self, X):
    Y = F.relu(self.bn1(self.conv1(X)))
    Y = self.bn2(self.conv2(Y))
    if self.conv3:
        X = self.conv3(X)
    return F.relu(Y + X)

#case 1
blk = Residual(3, 3)
X = torch.rand((4, 3, 6, 6))
blk(X).shape

#case 2
blk = Residual(3, 6, use_1x1conv=True, stride=2)
X = torch.rand((4, 3, 6, 6))
blk(X).shape

"""### Define the Model"""

#in resnet 4 residual block makes of 1 block, so resnet_18 = 4x4 + starting 7x7 conv + 1 FC 
#so resnet23= 4x5 + 7x7 conv + 2FC
def resnet_block(in_channels, out_channels, num_residuals, first_block=False):
  if first_block:
    assert in_channels == out_channels # same channels for in and out
  blk = []
  for i in range(num_residuals):
    if i==0 and not first_block:
      blk.append(Residual(in_channels, out_channels, use_1x1conv=True, stride=2))
    else:
      blk.append(Residual(out_channels, out_channels))
  
  return nn.Sequential(*blk)

class Resnet23(torch.nn.Module):
  def __init__(self, num_classes):
    super(Resnet23, self).__init__()
    self.block1 = nn.Sequential(
      nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1, stride=1),
      nn.BatchNorm2d(64),
      nn.ReLU(),
      nn.MaxPool2d(kernel_size=3, stride=2, padding=1    
    ))
    self.block2 = nn.Sequential(
        resnet_block(64, 64, 2, first_block=True))
    self.block3 = nn.Sequential(
        resnet_block(64, 128, 2)
    )
    self.block4 = nn.Sequential(
        resnet_block(128, 256, 2)
    )
    self.block5 = nn.Sequential(
        resnet_block(256, 512, 4)
    )
    self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
    self.fc1 = nn.Linear(512, 1024)
    self.fc2 = nn.Linear(1024, 10)

  def forward(self,X):
    logits = self.block1(X)
    logits = self.block2(logits)
    logits = self.block3(logits)
    logits = self.block4(logits)
    logits = self.block5(logits)
    logits = self.avgpool(logits)
    logits = logits.reshape(X.shape[0], -1)
    logits = self.fc1(logits)
    logits = self.fc2(logits)
    probas = F.softmax(logits,dim = 1)
    return logits, probas

X = torch.rand((2, 3, 32, 32)).to(device)
model = Resnet23(10).to(device)
print(model)
y = model(X)
print(y)

learning_rate = 0.01

optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)
# optimizer = optim.Adam(model.parameters(), lr=1e-3)
# optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.9, weight_decay=5e-4)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=40, gamma=0.1, verbose = True)
def compute_accuracy_and_loss(model, data_loader, device):
    correct_pred, num_examples = 0, 0
    cross_entropy = 0.
    for i, (features, targets) in enumerate(data_loader):
            
        features = features.to(device)
        targets = targets.to(device)

        logits, probas = model(features)
        cross_entropy += F.cross_entropy(logits, targets).item()
        _, predicted_labels = torch.max(probas, 1)
        num_examples += targets.size(0)
        correct_pred += (predicted_labels == targets).sum()
    return correct_pred.float()/num_examples * 100, cross_entropy/num_examples
    

import time
start_time = time.time()

epochs = 120
train_losses = []
val_losses = []
train_accs = []
val_accs = []


for i in range(epochs):
    model.train()
    
    # Run the training batches
    for batch_idx, (X_train, y_train) in enumerate(train_loader):#50000/64 = 782 batches

        X_train, y_train = X_train.to(device), y_train.to(device)
        # Apply the model
        logits, probas = model(X_train)
        loss = F.cross_entropy(logits, y_train)
        
        # Update parameters
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        if not batch_idx % 300:
            print (f'Epoch: {i+1:03d}/{epochs:03d} | '
                   f'Batch {batch_idx:03d}/{len(train_loader):03d} |' 
                   f' Cost: {loss:.4f}')
        
    scheduler.step()
    model.eval()
    # Run the testing batches
    with torch.no_grad():
        train_acc, train_loss = compute_accuracy_and_loss(model, train_loader, device=device)
        valid_acc, valid_loss = compute_accuracy_and_loss(model, valid_loader, device=device)
        train_accs.append(train_acc)
        val_accs.append(valid_acc)
        train_losses.append(train_loss)
        val_losses.append(valid_loss)
        print(f'Epoch: {i+1:03d}/{epochs:03d} Train Acc.: {train_acc:.2f}%'
              f' | Validation Acc.: {valid_acc:.2f}%')
print(f'\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed

model.eval()
with torch.set_grad_enabled(False): # save memory during inference
    test_acc, test_loss = compute_accuracy_and_loss(model, test_loader, device)
    print(f'Test accuracy: {test_acc:.2f}%')

plt.plot(range(1, epochs+1), train_losses, label='Training loss')
plt.plot(range(1, epochs+1), val_losses, label='Validation loss')
plt.title('Loss at the end of each epoch')
plt.xlabel('epochs')
plt.legend();

plt.plot(range(1, epochs+1), train_accs, label='Training accuracy')
plt.plot(range(1, epochs+1), val_accs, label='Validation accuracy')
plt.title('Accuracy at the end of each epoch')
plt.xlabel('epochs')
plt.legend()